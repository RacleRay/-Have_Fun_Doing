{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "快速图像风格迁移，实现原理在图像迁移的算法分析和提高部分做了简要介绍。\n",
    "\n",
    "简单来讲就是将训练网络对raw picture所做的变化过程（或者说函数，或者是非线性变换方法）由一个transfer net来代替。\n",
    "\n",
    "相当于用这个transfer net来记录了进行某一个特定style transfer所做的所有关键变换方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而在 infer 时只需要使用这个 transfer net ，而不用再次训练 整个网络。\n",
    "\n",
    "模型分为两个部分：  \n",
    "- transfer net\n",
    "- loss train net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一个不同点在于：训练 transfer net 需要大量的 content picture，才能拟合出合适的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "reference：\n",
    "1. https://github.com/lengstrom/fast-style-transfer\n",
    "2. https://github.com/hzy46/fast-neural-style-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset：http://cocodataset.org/#download   2014 Train images [83K/13GB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T12:27:27.657436Z",
     "start_time": "2019-10-23T12:26:54.288802Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.io\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from imageio import imread, imsave\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T12:39:15.898171Z",
     "start_time": "2019-10-23T12:39:15.891191Z"
    }
   },
   "outputs": [],
   "source": [
    "def resize_and_crop(image, image_size):\n",
    "    \"\"\"picture处理\"\"\"\n",
    "    h = image.shape[0]\n",
    "    w = image.shape[1]\n",
    "    \n",
    "    if h > w:\n",
    "        image = image[h // 2 - w // 2: h // 2 + w // 2, :, :]\n",
    "    else:\n",
    "        image = image[:, w // 2 - h // 2: w // 2 + h // 2, :]    \n",
    "    \n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 content picture\n",
    "X_data = []\n",
    "image_size = 256\n",
    "paths = glob.glob('train2014/*.jpg')\n",
    "\n",
    "for i in tqdm_notebook(range(len(paths))):\n",
    "    path = paths[i]\n",
    "    image = imread(path)\n",
    "    if len(image.shape) < 3:  # 黑白图片，删除\n",
    "        continue\n",
    "    X_data.append(resize_and_crop(image, image_size))\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "print(X_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature map extraction VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of the VGG19 model:\n",
    "\t- 0 is conv1_1 (3, 3, 3, 64)\n",
    "\t- 1 is relu\n",
    "\t- 2 is conv1_2 (3, 3, 64, 64)\n",
    "\t- 3 is relu    \n",
    "\t- 4 is maxpool\n",
    "\t- 5 is conv2_1 (3, 3, 64, 128)\n",
    "\t- 6 is relu\n",
    "\t- 7 is conv2_2 (3, 3, 128, 128)\n",
    "\t- 8 is relu\n",
    "\t- 9 is maxpool\n",
    "\t- 10 is conv3_1 (3, 3, 128, 256)\n",
    "\t- 11 is relu\n",
    "\t- 12 is conv3_2 (3, 3, 256, 256)\n",
    "\t- 13 is relu\n",
    "\t- 14 is conv3_3 (3, 3, 256, 256)\n",
    "\t- 15 is relu\n",
    "\t- 16 is conv3_4 (3, 3, 256, 256)\n",
    "\t- 17 is relu\n",
    "\t- 18 is maxpool\n",
    "\t- 19 is conv4_1 (3, 3, 256, 512)\n",
    "\t- 20 is relu\n",
    "\t- 21 is conv4_2 (3, 3, 512, 512)\n",
    "\t- 22 is relu\n",
    "\t- 23 is conv4_3 (3, 3, 512, 512)\n",
    "\t- 24 is relu\n",
    "\t- 25 is conv4_4 (3, 3, 512, 512)\n",
    "\t- 26 is relu\n",
    "\t- 27 is maxpool\n",
    "\t- 28 is conv5_1 (3, 3, 512, 512)\n",
    "\t- 29 is relu\n",
    "\t- 30 is conv5_2 (3, 3, 512, 512)\n",
    "\t- 31 is relu\n",
    "\t- 32 is conv5_3 (3, 3, 512, 512)\n",
    "\t- 33 is relu\n",
    "\t- 34 is conv5_4 (3, 3, 512, 512)\n",
    "\t- 35 is relu\n",
    "\t- 36 is maxpool\n",
    "\t- 37 is fullyconnected (7, 7, 512, 4096)\n",
    "\t- 38 is relu\n",
    "\t- 39 is fullyconnected (1, 1, 4096, 4096)\n",
    "\t- 40 is relu\n",
    "\t- 41 is fullyconnected (1, 1, 4096, 1000)\n",
    "\t- 42 is softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T12:48:06.637026Z",
     "start_time": "2019-10-23T12:48:01.918675Z"
    }
   },
   "outputs": [],
   "source": [
    "# 预训练model参数\n",
    "# http://www.vlfeat.org/matconvnet/pretrained/\n",
    "vgg = scipy.io.loadmat('imagenet-vgg-verydeep-19.mat')\n",
    "vgg_layers = vgg['layers']\n",
    "\n",
    "def vgg_endpoints(inputs, reuse=None):\n",
    "    \"\"\"定义出vgg19网络，加载预训练参数，计算图像特征\n",
    "\n",
    "    return: graph--网络及其参数，dict\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('endpoints', reuse=reuse):\n",
    "\n",
    "        def _weights(layer, expected_layer_name):\n",
    "            W = vgg_layers[0][layer][0][0][2][0][0]\n",
    "            b = vgg_layers[0][layer][0][0][2][0][1]\n",
    "            layer_name = vgg_layers[0][layer][0][0][0][0]\n",
    "            assert layer_name == expected_layer_name\n",
    "            return W, b\n",
    "\n",
    "        def _conv2d_relu(prev_layer, layer, layer_name):\n",
    "            W, b = _weights(layer, layer_name)\n",
    "            W = tf.constant(W)\n",
    "            b = tf.constant(np.reshape(b, (b.size)))\n",
    "            return tf.nn.relu(tf.nn.conv2d(\n",
    "                    prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b)\n",
    "\n",
    "        def _avgpool(prev_layer):\n",
    "            return tf.nn.avg_pool(prev_layer,\n",
    "                                  ksize=[1, 2, 2, 1],\n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "\n",
    "        graph = {}\n",
    "        graph['conv1_1'] = _conv2d_relu(inputs, 0, 'conv1_1')\n",
    "        graph['conv1_2'] = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
    "        graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "        graph['conv2_1'] = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
    "        graph['conv2_2'] = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
    "        graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "        graph['conv3_1'] = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
    "        graph['conv3_2'] = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
    "        graph['conv3_3'] = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
    "        graph['conv3_4'] = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
    "        graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "        graph['conv4_1'] = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
    "        graph['conv4_2'] = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
    "        graph['conv4_3'] = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
    "        graph['conv4_4'] = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
    "        graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "        graph['conv5_1'] = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
    "        graph['conv5_2'] = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
    "        graph['conv5_3'] = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
    "        graph['conv5_4'] = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
    "        graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### style gram matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:13:43.044898Z",
     "start_time": "2019-10-23T13:13:22.519840Z"
    }
   },
   "outputs": [],
   "source": [
    "style_images = glob.glob('styles/*.jpg')\n",
    "# params\n",
    "style_index = 0  # 图片index\n",
    "image_size = 256\n",
    "STYLE_LAYERS = ['conv1_2', 'conv2_2', 'conv3_3', 'conv4_3']  # 提取的layer tensor\n",
    "\n",
    "def gram_matix(style_index, style_images, image_size, STYLE_LAYERS):\n",
    "    \"\"\"计算STYLE_LAYERS中对应层的gram_matix，并返回style_features字典\"\"\"\n",
    "    X_style_data = resize_and_crop(imread(style_images[style_index]), \n",
    "                                                       image_size)\n",
    "    X_style_data = np.expand_dims(X_style_data, 0)\n",
    "    # print(X_style_data.shape)\n",
    "    \n",
    "    # MEAN_VALUES来自google net的大量图片统计\n",
    "    MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))\n",
    "\n",
    "    X_style = tf.placeholder(dtype=tf.float32, \n",
    "                                           shape=X_style_data.shape, \n",
    "                                           name='X_style')\n",
    "    style_endpoints = vgg_endpoints(X_style - MEAN_VALUES)\n",
    "    \n",
    "    style_features = {}\n",
    "    sess = tf.Session()\n",
    "    for layer_name in STYLE_LAYERS:\n",
    "        features = sess.run(style_endpoints[layer_name], \n",
    "                                        feed_dict={X_style: X_style_data})\n",
    "        \n",
    "        # 计算gram matix\n",
    "        # features.shape[3]：channels of feature map \n",
    "        features = np.reshape(features, (-1, features.shape[3]))\n",
    "        # 不同channel之间求点积，获得相关性\n",
    "        # features.size：总元素个数\n",
    "        gram = np.matmul(features.T, features) / features.size\n",
    "        style_features[layer_name] = gram\n",
    "        \n",
    "    return style_features\n",
    "\n",
    "style_features = gram_matix(style_index, style_images, image_size, STYLE_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transfer net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:29:14.274912Z",
     "start_time": "2019-10-23T13:29:14.267898Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "batch_size = 4\n",
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1, 1, 1, 3))\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, None, None, 3], name='X')\n",
    "k_initializer = tf.truncated_normal_initializer(0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:24:01.559379Z",
     "start_time": "2019-10-23T13:24:01.549405Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def conv2d(inputs, filters, kernel_size, strides):\n",
    "    p = int(kernel_size / 2)\n",
    "    # 先填充：在width和height维度，将边缘p个值，对称reflect到填充位置\n",
    "    h0 = tf.pad(inputs, \n",
    "                       [[0, 0], [p, p], [p, p], [0, 0]], \n",
    "                       mode='reflect')\n",
    "    return tf.layers.conv2d(inputs=h0, \n",
    "                                          filters=filters, \n",
    "                                          kernel_size=kernel_size, \n",
    "                                          strides=strides, \n",
    "                                          padding='valid', \n",
    "                                          kernel_initializer=k_initializer)\n",
    "\n",
    "def deconv2d(inputs, filters, kernel_size, strides):\n",
    "    \"\"\"不适用空洞填充的方法进行逆卷积，而是以下方式手动实现。因为tf的逆卷积函数，\n",
    "    在本模型中，会使得图像出现明显的网格\"\"\"\n",
    "    shape = tf.shape(inputs)\n",
    "    height, width = shape[1], shape[2]\n",
    "    # 先插值增大到2倍大\n",
    "    h0 = tf.image.resize_images(inputs, \n",
    "                                                  [height * strides * 2, width * strides * 2], \n",
    "                                                  tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    # 再卷积\n",
    "    return conv2d(h0, filters, kernel_size, strides)\n",
    "    \n",
    "def instance_norm(inputs):\n",
    "    \"\"\"每一个图片单独归一化\"\"\"\n",
    "    return tf.contrib.layers.instance_norm(inputs)\n",
    "\n",
    "def residual(inputs, filters, kernel_size):\n",
    "    \"\"\"residual block\"\"\"\n",
    "    h0 = relu(conv2d(inputs, filters, kernel_size, 1))\n",
    "    h0 = conv2d(h0, filters, kernel_size, 1)\n",
    "    return tf.add(inputs, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:29:51.686860Z",
     "start_time": "2019-10-23T13:29:17.758578Z"
    }
   },
   "outputs": [],
   "source": [
    "# variable_scope: 可以方便在infer时，提取该部分网络\n",
    "with tf.variable_scope('transformer', reuse=None):\n",
    "    # 先pad，后面裁剪，防止边缘效果的效果差\n",
    "    h0 = tf.pad(X - MEAN_VALUES, \n",
    "                       [[0, 0], [10, 10], [10, 10], [0, 0]], \n",
    "                       mode='reflect')\n",
    "    h0 = relu(instance_norm(conv2d(h0, 32, 9, 1)))\n",
    "    h0 = relu(instance_norm(conv2d(h0, 64, 3, 2)))\n",
    "    h0 = relu(instance_norm(conv2d(h0, 128, 3, 2)))  # 1/4 size\n",
    "\n",
    "    for i in range(5):\n",
    "        h0 = residual(h0, 128, 3)\n",
    "\n",
    "    h0 = relu(instance_norm(deconv2d(h0, 64, 3, 2)))  \n",
    "    h0 = relu(instance_norm(deconv2d(h0, 32, 3, 2)))  # 4 size\n",
    "    h0 = tf.nn.tanh(instance_norm(conv2d(h0, 3, 9, 1)))\n",
    "    \n",
    "    # [0, 255]\n",
    "    h0 = (h0 + 1) / 2 * 255.\n",
    "    \n",
    "    # [0, 10, 10, 0]: slice开始位置，[-1, shape[1] - 20, shape[2] - 20, -1]：slice结束位置\n",
    "    shape = tf.shape(h0)\n",
    "    g = tf.slice(h0, \n",
    "                      [0, 10, 10, 0], \n",
    "                      [-1, shape[1] - 20, shape[2] - 20, -1], \n",
    "                      name='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:32:10.995267Z",
     "start_time": "2019-10-23T13:32:09.152226Z"
    }
   },
   "outputs": [],
   "source": [
    "CONTENT_LAYER = 'conv3_3'\n",
    "\n",
    "# vgg特征提取\n",
    "# 原图\n",
    "content_endpoints = vgg_endpoints(X - MEAN_VALUES, True)\n",
    "# 生成图\n",
    "g_endpoints = vgg_endpoints(g - MEAN_VALUES, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:32:48.416151Z",
     "start_time": "2019-10-23T13:32:48.407170Z"
    }
   },
   "outputs": [],
   "source": [
    "# 损失计算\n",
    "def get_content_loss(endpoints_x, endpoints_y, layer_name):\n",
    "    x = endpoints_x[layer_name]\n",
    "    y = endpoints_y[layer_name]\n",
    "    return 2 * tf.nn.l2_loss(x - y) / tf.to_float(tf.size(x))\n",
    "\n",
    "content_loss = get_content_loss(content_endpoints, \n",
    "                                                     g_endpoints, \n",
    "                                                     CONTENT_LAYER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:38:38.762228Z",
     "start_time": "2019-10-23T13:38:38.446041Z"
    }
   },
   "outputs": [],
   "source": [
    "style_loss = []\n",
    "STYLE_LAYERS = ['conv1_2', 'conv2_2', 'conv3_3', 'conv4_3']  # 提取的layer tensor\n",
    "\n",
    "for layer_name in STYLE_LAYERS:\n",
    "    # 生成图片garm matrix计算\n",
    "    layer = g_endpoints[layer_name]\n",
    "    shape = tf.shape(layer)\n",
    "    bs, height, width, channel = shape[0], shape[1], shape[2], shape[3]\n",
    "    \n",
    "    features = tf.reshape(layer, (bs, height * width, channel))\n",
    "    gram = tf.matmul(tf.transpose(features, (0, 2, 1)), features) \n",
    "    gram /= tf.to_float(height * width * channel)\n",
    "    \n",
    "    # 原风格图片garm matrix\n",
    "    style_gram = style_features[layer_name]\n",
    "    \n",
    "    # loss\n",
    "    style_loss.append(2 * tf.nn.l2_loss(gram - style_gram) / tf.to_float(tf.size(layer)))\n",
    "\n",
    "style_loss = tf.reduce_sum(style_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 全变差正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:43:58.035030Z",
     "start_time": "2019-10-23T13:43:58.031012Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "content_weight = 1\n",
    "style_weight = 250\n",
    "total_variation_weight = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:44:38.424964Z",
     "start_time": "2019-10-23T13:44:38.418961Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_total_variation_loss(inputs):\n",
    "    \"\"\"相邻位置之差的L2 loss。若相邻位置变化太大，那么损失会很高。使图像平滑\"\"\"\n",
    "    h = inputs[:, :-1, :, :] - inputs[:, 1:, :, :]  # height方向上相邻位置之差\n",
    "    w = inputs[:, :, :-1, :] - inputs[:, :, 1:, :]  # width方向上相邻位置之差\n",
    "    return tf.nn.l2_loss(h) / tf.to_float(tf.size(h)) \\\n",
    "                + tf.nn.l2_loss(w) / tf.to_float(tf.size(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:44:39.352456Z",
     "start_time": "2019-10-23T13:44:39.323534Z"
    }
   },
   "outputs": [],
   "source": [
    "total_variation_loss = get_total_variation_loss(g)\n",
    "\n",
    "loss = content_weight * content_loss \\\n",
    "           + style_weight * style_loss \\\n",
    "           + total_variation_weight * total_variation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:50:18.992024Z",
     "start_time": "2019-10-23T13:50:18.938139Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "epochs = 2\n",
    "X_sample = imread('sample.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:48:05.719218Z",
     "start_time": "2019-10-23T13:48:03.442681Z"
    }
   },
   "outputs": [],
   "source": [
    "# 学习 transfer net的参数\n",
    "vars_t = [var for var in tf.trainable_variables() if var.name.startswith('transformer')]\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss, \n",
    "                                                                                                            var_list=vars_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:49:16.623028Z",
     "start_time": "2019-10-23T13:49:16.578148Z"
    }
   },
   "outputs": [],
   "source": [
    "style_name = style_images[style_index]\n",
    "style_name = style_name[style_name.find('\\\\') + 1:].rstrip('.jpg')\n",
    "OUTPUT_DIR = 'samples_%s' % style_name\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "tf.summary.scalar('losses/content_loss', content_loss)\n",
    "tf.summary.scalar('losses/style_loss', style_loss)\n",
    "tf.summary.scalar('losses/total_variation_loss', total_variation_loss)\n",
    "tf.summary.scalar('losses/loss', loss)\n",
    "tf.summary.scalar('weighted_losses/weighted_content_loss', \n",
    "                              content_weight * content_loss)\n",
    "tf.summary.scalar('weighted_losses/weighted_style_loss', \n",
    "                              style_weight * style_loss)\n",
    "tf.summary.scalar('weighted_losses/weighted_total_variation_loss', \n",
    "                              total_variation_weight * total_variation_loss)\n",
    "\n",
    "tf.summary.image('transformed', g)\n",
    "tf.summary.image('origin', X)\n",
    "\n",
    "summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "losses = []\n",
    "\n",
    "h_sample = X_sample.shape[0]\n",
    "w_sample = X_sample.shape[1]\n",
    "\n",
    "for e in range(epochs):\n",
    "    data_index = np.arange(X_data.shape[0])\n",
    "    np.random.shuffle(data_index)\n",
    "    X_data = X_data[data_index]\n",
    "    \n",
    "    for i in tqdm_notebook(range(X_data.shape[0] // batch_size)):\n",
    "        X_batch = X_data[i * batch_size: i * batch_size + batch_size]\n",
    "        \n",
    "        ls_, _ = sess.run([loss, optimizer], feed_dict={X: X_batch})\n",
    "        losses.append(ls_)\n",
    "        \n",
    "        if i > 0 and i % 20 == 0:\n",
    "            writer.add_summary(sess.run(summary,\n",
    "                                                            feed_dict={X: X_batch}), \n",
    "                                               e * X_data.shape[0] // batch_size + i)\n",
    "            writer.flush()\n",
    "        \n",
    "    print('Epoch %d Loss %f' % (e, np.mean(losses)))\n",
    "    losses = []  # reset losses temp\n",
    "\n",
    "    gen_img = sess.run(g, feed_dict={X: [X_sample]})[0]\n",
    "    gen_img = np.clip(gen_img, 0, 255)\n",
    "    \n",
    "    result = np.zeros((h_sample, w_sample * 2, 3))\n",
    "    # 原图\n",
    "    result[:, :w_sample, :] = X_sample / 255.\n",
    "    # 生成\n",
    "    result[:, w_sample:, :] = gen_img[:h_sample, :w_sample, :] / 255.\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(result)\n",
    "    plt.show()\n",
    "    \n",
    "    imsave(os.path.join(OUTPUT_DIR, 'sample_%d.jpg' % e), result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:54:13.511767Z",
     "start_time": "2019-10-23T13:54:11.075703Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from imageio import imread, imsave\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T13:54:17.598371Z",
     "start_time": "2019-10-23T13:54:17.594348Z"
    }
   },
   "outputs": [],
   "source": [
    "def the_current_time():\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(int(time.time()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T14:17:52.649673Z",
     "start_time": "2019-10-23T14:17:30.355778Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# restore model\n",
    "saver = tf.train.import_meta_graph('fast_style_transfer.meta')\n",
    "\n",
    "# get tensor\n",
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name('X:0')\n",
    "g = graph.get_tensor_by_name('transformer/g:0')\n",
    "\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# ['wave', 'rain', 'starry', 'scream', 'mosaic', 'muse']\n",
    "style = 'rain'\n",
    "\n",
    "# load params\n",
    "model = 'samples_%s' % style\n",
    "saver.restore(sess, tf.train.latest_checkpoint(model))\n",
    "\n",
    "content_image = '144924.png'\n",
    "result_image = '144924_%s.png' % style\n",
    "X_image = imread(content_image)\n",
    "\n",
    "the_current_time()\n",
    "\n",
    "# generate\n",
    "gen_img = sess.run(g, feed_dict={X: [X_image]})[0]\n",
    "gen_img = np.clip(gen_img, 0, 255) / 255.\n",
    "\n",
    "imsave(result_image, gen_img)\n",
    "\n",
    "the_current_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
