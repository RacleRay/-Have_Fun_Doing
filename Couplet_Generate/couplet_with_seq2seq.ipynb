{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seq2seq构建写对联AI\n",
    "### 代码参考：[seq2seq-couplet](https://github.com/wb14123/seq2seq-couplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题背景介绍\n",
    "对联又称对子，对仗工整，平仄协调，是一字一音的汉文语言独特的艺术形式，是中国传统文化瑰宝。对联的上下联有着非常工整的对应关系，我们可以尝试使用神经网络学习对应关系，进而完成对对联任务，而之前提到的seq2seq模型，是非常典型的序列映射学习模型，可以在本场景下使用。\n",
    "\n",
    "![](../img/couplet.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq对对联\n",
    "##### \\[稀牛学院 x 网易云课程\\]《AI工程师(自然语言处理方向)》课程资料 by [@寒小阳](https://blog.csdn.net/han_xiaoyang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里构建的对对联AI应用也是seq2seq模型，使用的就是我们在上一门中讲解到的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/[1]_seq2seq_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/[8]_seq2seq_8.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/attention_tensor_dance.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import random\n",
    "\n",
    "\n",
    "def padding_seq(seq):\n",
    "    \"\"\"padding每个输入sequence为最大的sequence长度\n",
    "    arg：seq of ids\n",
    "    return: results, padding到max_len的id list\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    max_len = 0\n",
    "    for s in seq:\n",
    "        if max_len < len(s):\n",
    "            max_len = len(s)\n",
    "    for i in range(0, len(seq)):\n",
    "        l = max_len - len(seq[i])\n",
    "        results.append(seq[i] + [0 for j in range(l)])\n",
    "    return results\n",
    "\n",
    "\n",
    "def encode_text(words, vocab_indices):\n",
    "    \"\"\"把文本序列映射为id序列\n",
    "    args: words, 输入对联中每个字组成的list\n",
    "          vocab_indices，词到id的dict\n",
    "    return：文本序列对应的id序列      \n",
    "    \"\"\"\n",
    "    return [vocab_indices[word] for word in words if word in vocab_indices]\n",
    "\n",
    "\n",
    "def decode_text(labels, vocabs, end_token='</s>'):\n",
    "    \"\"\"把id序列映射为文本序列\n",
    "    args: labels, decoder输出的预测结果list\n",
    "          vocab，id到词的dict\n",
    "    return：results，' '连接的预测文本\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx in labels:\n",
    "        word = vocabs[idx]\n",
    "        if word == end_token:\n",
    "            return ' '.join(results)\n",
    "        results.append(word)\n",
    "    return ' '.join(results)\n",
    "\n",
    "\n",
    "def read_vocab(vocab_file):\n",
    "    \"\"\"读取词表文件\n",
    "    return：vocabs，list包含文件中的所有字及<s>,</s>,','\n",
    "    \"\"\"\n",
    "    f = open(vocab_file, 'rb')\n",
    "    vocabs = [line.decode('utf-8')[:-1] for line in f]\n",
    "    f.close()\n",
    "    return vocabs\n",
    "\n",
    "\n",
    "class SeqReader():\n",
    "    \"\"\"输入序列读取类\"\"\"\n",
    "    def __init__(self,\n",
    "                 input_file,\n",
    "                 target_file,\n",
    "                 vocab_file,\n",
    "                 batch_size,\n",
    "                 queue_size=2048,\n",
    "                 worker_size=2,\n",
    "                 end_token='</s>',\n",
    "                 padding=True,\n",
    "                 max_len=50):\n",
    "        self.input_file = input_file\n",
    "        self.target_file = target_file\n",
    "        self.vocabs = read_vocab(vocab_file)\n",
    "        # 词到id的dict\n",
    "        self.vocab_indices = dict((c, i) for i, c in enumerate(self.vocabs))\n",
    "        self.batch_size = batch_size\n",
    "        self.padding = padding      \n",
    "        self.data_queue = Queue(queue_size)\n",
    "        self.worker_size = worker_size\n",
    "        self.end_token = end_token\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        with open(self.input_file, 'rb') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                pass\n",
    "            f.close()\n",
    "            self.single_lines = i + 1    # 输入文件总行数\n",
    "            \n",
    "        self.data_size = int(self.single_lines / batch_size) # batch总数\n",
    "        self.data_pos = 0     # 指针，self.data中的某一个索引\n",
    "        self._init_reader()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"多线程运行_init_reader()\"\"\"\n",
    "        for i in range(self.worker_size):\n",
    "            t = Thread(target=self._init_reader())\n",
    "            t.daemon = True   # 守护线程，后台运行\n",
    "            t.start()\n",
    "        return\n",
    "        \n",
    "    def read_single_data(self):\n",
    "        \"\"\"读取一组数据，\n",
    "        return:{\n",
    "                'in_seq': in_seq,\n",
    "                'in_seq_len': len(in_seq),\n",
    "                'target_seq': target_seq,\n",
    "                'target_seq_len': len(target_seq) - 1\n",
    "        }\n",
    "        \"\"\"\n",
    "        if self.data_pos >= len(self.data):\n",
    "            random.shuffle(self.data)\n",
    "            self.data_pos = 0\n",
    "        result = self.data[self.data_pos]\n",
    "        self.data_pos += 1\n",
    "        return result\n",
    "\n",
    "    def read(self):\n",
    "        \"\"\"batch生成器\n",
    "        yield：batch，dict类型，{\n",
    "                'in_seq': [[seq1], [seq2], ...],\n",
    "                'in_seq_len': [int, int, ...],\n",
    "                'target_seq': [[seq1], [seq2], ...],\n",
    "                'target_seq_len': [int, int, ...]\n",
    "                }\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            batch = {\n",
    "                'in_seq': [],\n",
    "                'in_seq_len': [],\n",
    "                'target_seq': [],\n",
    "                'target_seq_len': []\n",
    "            }\n",
    "            for i in range(0, self.batch_size):\n",
    "                item = self.read_single_data()\n",
    "                batch['in_seq'].append(item['in_seq'])\n",
    "                batch['in_seq_len'].append(item['in_seq_len'])\n",
    "                batch['target_seq'].append(item['target_seq'])\n",
    "                batch['target_seq_len'].append(item['target_seq_len'])\n",
    "            if self.padding:\n",
    "                batch['in_seq'] = padding_seq(batch['in_seq'])\n",
    "                batch['target_seq'] = padding_seq(batch['target_seq'])\n",
    "            yield batch\n",
    "\n",
    "    def _init_reader(self):\n",
    "        \"\"\"文件读取，预处理数据格式\n",
    "        self.data保存了转化为id的每组input sequence、target sequence的dict，储\n",
    "        存在list中\n",
    "        \"\"\"\n",
    "        self.data = []   # 初始化输出数据为空list\n",
    "        input_f = open(self.input_file, 'rb')\n",
    "        target_f = open(self.target_file, 'rb')\n",
    "        \n",
    "        for input_line in input_f:\n",
    "            input_line = input_line.decode('utf-8')[:-1]\n",
    "            # target_line按行读取\n",
    "            target_line = target_f.readline().decode('utf-8')[:-1]\n",
    "            # 文本以' '为每个字的分隔符 \n",
    "            input_words = [x for x in input_line.split(' ') if x != '']\n",
    "            \n",
    "            if len(input_words) >= self.max_len:\n",
    "                input_words = input_words[:self.max_len - 1]\n",
    "            input_words.append(self.end_token)\n",
    "            \n",
    "            target_words = [x for x in target_line.split(' ') if x != '']\n",
    "            if len(target_words) >= self.max_len:\n",
    "                target_words = target_words[:self.max_len - 1]\n",
    "            target_words = ['<s>',] + target_words # 加入开始符\n",
    "            target_words.append(self.end_token)    # 加入结束符\n",
    "            \n",
    "            in_seq = encode_text(input_words, self.vocab_indices)\n",
    "            target_seq = encode_text(target_words, self.vocab_indices)\n",
    "            self.data.append({\n",
    "                'in_seq': in_seq,\n",
    "                'in_seq_len': len(in_seq),\n",
    "                'target_seq': target_seq,\n",
    "                'target_seq_len': len(target_seq) - 1 # <s>不计入\n",
    "            })\n",
    "            \n",
    "        input_f.close()\n",
    "        target_f.close()\n",
    "        self.data_pos = len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Python implementation of BLEU and smooth-BLEU.\n",
    "\n",
    "This module provides a Python implementation of BLEU and smooth-BLEU.\n",
    "Smooth BLEU is computed following the method outlined in the paper:\n",
    "Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\n",
    "evaluation metrics for machine translation. COLING 2004.\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "    \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "\n",
    "    Args:\n",
    "        segment: text segment from which n-grams will be extracted.\n",
    "        max_order: maximum length in tokens of the n-grams returned by this\n",
    "            methods.\n",
    "\n",
    "    Returns:\n",
    "        The Counter containing all n-grams upto max_order in segment\n",
    "        with a count of how many times each n-gram occurred.\n",
    "        (all n-grams upto max_order)，keys：n-gram，value：count\n",
    "    \"\"\"\n",
    "    ngram_counts = collections.Counter()\n",
    "    for order in range(1, max_order + 1):\n",
    "        for i in range(0, len(segment) - order + 1):\n",
    "            ngram = tuple(segment[i:i + order])\n",
    "            ngram_counts[ngram] += 1\n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus,\n",
    "                 translation_corpus,\n",
    "                 max_order=4,\n",
    "                 smooth=False):\n",
    "    \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "\n",
    "    Args:\n",
    "        reference_corpus: list of lists of references for each translation. Each\n",
    "            reference should be tokenized into a list of tokens.\n",
    "        translation_corpus: list of translations to score. Each translation\n",
    "            should be tokenized into a list of tokens.\n",
    "        max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "        smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "\n",
    "    Returns:\n",
    "        3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "        precisions and brevity penalty.\n",
    "        bleu：float，翻译句子的bleu得分, \n",
    "        precisions：list, 包含每种ngram的准确率，\n",
    "        bp：brevity penalty, 短句惩罚系数，\n",
    "        ratio：translation_length / min(reference_length), \n",
    "        translation_length：int，翻译长度, \n",
    "        reference_length：int，最短的reference长度\n",
    "    \"\"\"\n",
    "    matches_by_order = [0] * max_order\n",
    "    possible_matches_by_order = [0] * max_order\n",
    "    reference_length = 0\n",
    "    translation_length = 0\n",
    "    \n",
    "    for (references, translation) in zip(reference_corpus, translation_corpus):\n",
    "        reference_length += min(len(r) for r in references)\n",
    "        translation_length += len(translation)\n",
    "\n",
    "        merged_ref_ngram_counts = collections.Counter()\n",
    "        # 同时考虑多个references\n",
    "        for reference in references:\n",
    "            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)# 位或\n",
    "        translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "        overlap = translation_ngram_counts & merged_ref_ngram_counts    # 位与\n",
    "        # matches_by_order：｛len(ngram)：sum of counts｝\n",
    "        for ngram in overlap:\n",
    "            matches_by_order[len(ngram) - 1] += overlap[ngram]\n",
    "        # possible_matches_by_order(可匹配n-gram总数)：\n",
    "        # ｛len(ngram)：sum of each ngram｝\n",
    "        for order in range(1, max_order + 1):\n",
    "            possible_matches = len(translation) - order + 1\n",
    "            if possible_matches > 0:\n",
    "                possible_matches_by_order[order - 1] += possible_matches\n",
    "\n",
    "    precisions = [0] * max_order\n",
    "    for i in range(0, max_order):\n",
    "        if smooth:\n",
    "            precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                             (possible_matches_by_order[i] + 1.))\n",
    "        else:\n",
    "            if possible_matches_by_order[i] > 0:\n",
    "                precisions[i] = (\n",
    "                    float(matches_by_order[i]) / possible_matches_by_order[i])\n",
    "            else:\n",
    "                precisions[i] = 0.0\n",
    "\n",
    "    if min(precisions) > 0:\n",
    "        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "        geo_mean = math.exp(p_log_sum)\n",
    "    else:\n",
    "        geo_mean = 0\n",
    "\n",
    "    # 翻译长度惩罚（对较短的翻译基于较大的惩罚，以防止短翻译准确率会更高的问题）\n",
    "    ratio = float(translation_length) / reference_length\n",
    "    if ratio > 1.0:\n",
    "        bp = 1.\n",
    "    else:\n",
    "        bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "    bleu = geo_mean * bp\n",
    "\n",
    "    return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.layers import core as layers_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLayeredCell(layer_size,\n",
    "                   num_units,\n",
    "                   input_keep_prob,\n",
    "                   output_keep_prob=1.0):\n",
    "    '''多层rnn单元构造'''\n",
    "    return rnn.MultiRNNCell([\n",
    "        rnn.DropoutWrapper(\n",
    "            tf.nn.rnn_cell.LSTMCell(\n",
    "                name='basic_lstm_cell', num_units=num_units), input_keep_prob,\n",
    "            output_keep_prob) for i in range(layer_size)\n",
    "    ])\n",
    "\n",
    "\n",
    "def bi_encoder(embed_input, in_seq_len, num_units, layer_size,\n",
    "               input_keep_prob):\n",
    "    '''双向rnn编码器\n",
    "    embed_input：embeddirding后的输入序列\n",
    "    num_units：隐藏层单元数\n",
    "    layer_size：rnn层数\n",
    "    return: \n",
    "      bidirectional_dynamic_rnn不同于bidirectional_rnn，结果级联分层输出，可\n",
    "      concat到一起\n",
    "      encoder_output: 每个timestep输出，每层输出按最后一维concat到一起\n",
    "      encoder_state：每层的final state，(output_state_fw, output_state_bw)\n",
    "    '''\n",
    "    # encode input into a vector\n",
    "    bi_layer_size = int(layer_size / 2)\n",
    "    encode_cell_fw = getLayeredCell(bi_layer_size, num_units, input_keep_prob)\n",
    "    encode_cell_bw = getLayeredCell(bi_layer_size, num_units, input_keep_prob)\n",
    "    bi_encoder_output, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        cell_fw=encode_cell_fw,\n",
    "        cell_bw=encode_cell_bw,\n",
    "        inputs=embed_input,\n",
    "        sequence_length=in_seq_len,\n",
    "        dtype=embed_input.dtype,\n",
    "        time_major=False)\n",
    "\n",
    "    # concat encode output and state\n",
    "    encoder_output = tf.concat(bi_encoder_output, -1)\n",
    "    encoder_state = []\n",
    "    for layer_id in range(bi_layer_size):\n",
    "        encoder_state.append(bi_encoder_state[0][layer_id])\n",
    "        encoder_state.append(bi_encoder_state[1][layer_id])\n",
    "    encoder_state = tuple(encoder_state)\n",
    "    return encoder_output, encoder_state\n",
    "\n",
    "\n",
    "def attention_decoder_cell(encoder_output, in_seq_len, num_units, layer_size,\n",
    "                           input_keep_prob):\n",
    "    '''attention decoder\n",
    "    return: 加入attention_mechanim的decoder cell\n",
    "    '''\n",
    "    attention_mechanim = tf.contrib.seq2seq.BahdanauAttention(\n",
    "        num_units, encoder_output, in_seq_len, normalize=True)\n",
    "    # attention_mechanim = tf.contrib.seq2seq.LuongAttention(num_units,\n",
    "    #         encoder_output, in_seq_len, scale = True)\n",
    "    cell = getLayeredCell(layer_size, num_units, input_keep_prob)\n",
    "    cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell, attention_mechanim, attention_layer_size=num_units)\n",
    "    return cell\n",
    "\n",
    "\n",
    "def decoder_projection(output, output_size):\n",
    "    return tf.layers.dense(\n",
    "        output,\n",
    "        output_size,\n",
    "        activation=None,\n",
    "        use_bias=False,\n",
    "        name='output_mlp')\n",
    "\n",
    "\n",
    "def train_decoder(encoder_output, in_seq_len, target_seq, target_seq_len,\n",
    "                  encoder_state, num_units, layers, embedding, output_size,\n",
    "                  input_keep_prob, projection_layer):\n",
    "    '''只进行train过程'''\n",
    "    decoder_cell = attention_decoder_cell(encoder_output, in_seq_len,\n",
    "                                          num_units, layers, input_keep_prob)\n",
    "    batch_size = tf.shape(in_seq_len)[0]\n",
    "    init_state = decoder_cell.zero_state(\n",
    "        batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "        target_seq, target_seq_len, time_major=False)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, init_state, output_layer=projection_layer)\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, maximum_iterations=100)\n",
    "    return outputs.rnn_output\n",
    "\n",
    "\n",
    "def infer_decoder(encoder_output, in_seq_len, encoder_state, num_units, layers,\n",
    "                  embedding, output_size, input_keep_prob, projection_layer):\n",
    "    '''seq2seq函数中可以使用beamsearch方法来进行decoder'''\n",
    "    decoder_cell = attention_decoder_cell(encoder_output, in_seq_len,\n",
    "                                          num_units, layers, input_keep_prob)\n",
    "\n",
    "    batch_size = tf.shape(in_seq_len)[0]\n",
    "    init_state = decoder_cell.zero_state(\n",
    "        batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "\n",
    "    # TODO: start tokens and end tokens are hard code\n",
    "    \"\"\"\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding, tf.fill([batch_size], 0), 1)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper,\n",
    "            init_state, output_layer=projection_layer)\n",
    "    \"\"\"\n",
    "\n",
    "    decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "        cell=decoder_cell,\n",
    "        embedding=embedding,\n",
    "        start_tokens=tf.fill([batch_size], 0),\n",
    "        end_token=1,\n",
    "        initial_state=init_state,\n",
    "        beam_width=10,\n",
    "        output_layer=projection_layer,\n",
    "        length_penalty_weight=1.0)\n",
    "\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, maximum_iterations=100)\n",
    "    return outputs.sample_id\n",
    "\n",
    "\n",
    "def seq2seq(in_seq, in_seq_len, target_seq, target_seq_len, vocab_size,\n",
    "            num_units, layers, dropout):\n",
    "    \"\"\"seq2seq模型建立\n",
    "    return: training -- outputs.rnn_output: rnn输出的概率分布结果\n",
    "            infering -- outputs.sample_id：输出词id\n",
    "    \"\"\"\n",
    "    in_shape = tf.shape(in_seq)\n",
    "    batch_size = in_shape[0]\n",
    "    \n",
    "    # 训练开启dropout，预测不开启\n",
    "    if target_seq != None:\n",
    "        input_keep_prob = 1 - dropout\n",
    "    else:\n",
    "        input_keep_prob = 1\n",
    "    \n",
    "    # 全连接层输出预测\n",
    "    projection_layer = layers_core.Dense(vocab_size, use_bias=False)\n",
    "\n",
    "    # embedding input and target sequence\n",
    "    with tf.device('/cpu:0'):\n",
    "        embedding = tf.get_variable(\n",
    "            name='embedding', shape=[vocab_size, num_units])\n",
    "    embed_input = tf.nn.embedding_lookup(embedding, in_seq, name='embed_input')\n",
    "\n",
    "    # encode and decode\n",
    "    encoder_output, encoder_state = bi_encoder(\n",
    "        embed_input, in_seq_len, num_units, layers, input_keep_prob)\n",
    "\n",
    "    decoder_cell = attention_decoder_cell(encoder_output, in_seq_len,\n",
    "                                          num_units, layers, input_keep_prob)\n",
    "    batch_size = tf.shape(in_seq_len)[0]\n",
    "    # decoder初始化，权重初始化，并且将cell state初始化为encoder的final state\n",
    "    init_state = decoder_cell.zero_state(\n",
    "        batch_size, tf.float32).clone(cell_state=encoder_state)\n",
    "\n",
    "    if target_seq != None:\n",
    "        embed_target = tf.nn.embedding_lookup(\n",
    "            embedding, target_seq, name='embed_target')\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            embed_target, target_seq_len, time_major=False)\n",
    "    else:\n",
    "        # TODO: start tokens and end tokens are hard code\n",
    "        # 0，1分别应对应句子的起始符id和终止符id\n",
    "        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding, tf.fill([batch_size], 0), 1)\n",
    "        \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "        decoder_cell, helper, init_state, output_layer=projection_layer)\n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "        decoder, maximum_iterations=100)\n",
    "    \n",
    "    if target_seq != None:\n",
    "        return outputs.rnn_output\n",
    "    else:\n",
    "        return outputs.sample_id\n",
    "\n",
    "\n",
    "def seq_loss(output, target, seq_len):\n",
    "    '''计算损失\n",
    "    target：包括一个起始符\n",
    "    '''\n",
    "    target = target[:, 1:]\n",
    "    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=output, labels=target)\n",
    "    batch_size = tf.shape(target)[0]\n",
    "    # 不每个句子对都达到max_timestep，排除多余字符带来的损失\n",
    "    loss_mask = tf.sequence_mask(seq_len, tf.shape(output)[1])\n",
    "    cost = cost * tf.to_float(loss_mask)\n",
    "    return tf.reduce_sum(cost) / tf.to_float(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import path\n",
    "import random\n",
    "\n",
    "\n",
    "class Model():\n",
    "    \"\"\"创建模型类\"\"\"\n",
    "    def __init__(self,\n",
    "                 train_input_file,\n",
    "                 train_target_file,\n",
    "                 test_input_file,\n",
    "                 test_target_file,\n",
    "                 vocab_file,\n",
    "                 num_units,\n",
    "                 layers,\n",
    "                 dropout,\n",
    "                 batch_size,\n",
    "                 learning_rate,\n",
    "                 output_dir,\n",
    "                 save_step=100,\n",
    "                 eval_step=1000,\n",
    "                 param_histogram=False,\n",
    "                 restore_model=False,\n",
    "                 init_train=True,\n",
    "                 init_infer=False):\n",
    "        self.num_units = num_units  # 单个RNN结构中，神经元数目\n",
    "        self.layers = layers\n",
    "        self.dropout = dropout\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_step = save_step\n",
    "        self.eval_step = eval_step\n",
    "        self.param_histogram = param_histogram\n",
    "        self.restore_model = restore_model  # boolen\n",
    "        self.init_train = init_train\n",
    "        self.init_infer = init_infer\n",
    "\n",
    "        if init_train:\n",
    "            self.train_reader = SeqReader(train_input_file, train_target_file,\n",
    "                                          vocab_file, batch_size)\n",
    "            self.train_reader.start()  # 多线程\n",
    "            self.train_data = self.train_reader.read()  # yield batch\n",
    "            self.eval_reader = SeqReader(test_input_file, test_target_file,\n",
    "                                         vocab_file, batch_size)\n",
    "            self.eval_reader.start()\n",
    "            self.eval_data = self.eval_reader.read()\n",
    "\n",
    "        self.model_file = path.join(output_dir, 'model.ckpl')\n",
    "        self.log_writter = tf.summary.FileWriter(output_dir)\n",
    "\n",
    "        if init_train:\n",
    "            self._init_train()\n",
    "            self._init_eval()\n",
    "\n",
    "        if init_infer:\n",
    "            self.infer_vocabs = reader.read_vocab(vocab_file)\n",
    "            self.infer_vocab_indices = dict(\n",
    "                (c, i) for i, c in enumerate(self.infer_vocabs))\n",
    "            self._init_infer()\n",
    "            self.reload_infer_model()\n",
    "\n",
    "    def gpu_session_config(self):\n",
    "        # allow_growth: 刚一开始分配少量的GPU容量，然后按需慢慢的增加\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return config\n",
    "\n",
    "    def _init_train(self):\n",
    "        '''初始化训练会话'''\n",
    "        self.train_graph = tf.Graph()\n",
    "        with self.train_graph.as_default():\n",
    "            # 输入\n",
    "            self.train_in_seq = tf.placeholder(\n",
    "                tf.int32, shape=[self.batch_size, None])\n",
    "            self.train_in_seq_len = tf.placeholder(\n",
    "                tf.int32, shape=[self.batch_size])\n",
    "            self.train_target_seq = tf.placeholder(\n",
    "                tf.int32, shape=[self.batch_size, None])\n",
    "            self.train_target_seq_len = tf.placeholder(\n",
    "                tf.int32, shape=[self.batch_size])\n",
    "            \n",
    "            # 输出\n",
    "            output = seq2seq(self.train_in_seq, self.train_in_seq_len,\n",
    "                             self.train_target_seq, self.train_target_seq_len,\n",
    "                             len(self.train_reader.vocabs), self.num_units,\n",
    "                             self.layers, self.dropout)\n",
    "            self.train_output = tf.argmax(tf.nn.softmax(output), 2)\n",
    "            \n",
    "            # 损失\n",
    "            self.loss = seq_loss(output, self.train_target_seq,\n",
    "                                 self.train_target_seq_len)\n",
    "            \n",
    "            # 梯度截断\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, 0.5)\n",
    "            self.train_op = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate).apply_gradients(\n",
    "                    list(zip(clipped_gradients, params)))\n",
    "            \n",
    "            # 变量统计输出历史变化情况\n",
    "            if self.param_histogram:\n",
    "                for v in tf.trainable_variables():\n",
    "                    tf.summary.histogram('train_' + v.name, v)\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            self.train_summary = tf.summary.merge_all()\n",
    "            \n",
    "            self.train_init = tf.global_variables_initializer()\n",
    "            self.train_saver = tf.train.Saver()\n",
    "        \n",
    "        self.train_session = tf.Session(\n",
    "            graph=self.train_graph, config=self.gpu_session_config())\n",
    "\n",
    "    def _init_eval(self):\n",
    "        '''初始化测试操作'''\n",
    "        self.eval_graph = tf.Graph()\n",
    "        with self.eval_graph.as_default():\n",
    "            \n",
    "            self.eval_in_seq = tf.placeholder(\n",
    "                tf.int32, shape=[self.batch_size, None])\n",
    "            self.eval_in_seq_len = tf.placeholder(\n",
    "                tf.int32, shape=[self.batch_size])\n",
    "            \n",
    "            self.eval_output = seq2seq(\n",
    "                self.eval_in_seq, self.eval_in_seq_len, None, None,\n",
    "                len(self.eval_reader.vocabs), self.num_units, self.layers,\n",
    "                self.dropout)\n",
    "            \n",
    "            if self.param_histogram:\n",
    "                for v in tf.trainable_variables():\n",
    "                    tf.summary.histogram('eval_' + v.name, v)\n",
    "            self.eval_summary = tf.summary.merge_all()\n",
    "            self.eval_saver = tf.train.Saver()\n",
    "        \n",
    "        self.eval_session = tf.Session(\n",
    "            graph=self.eval_graph, config=self.gpu_session_config())\n",
    "\n",
    "    def _init_infer(self):\n",
    "        '''初始化推断'''\n",
    "        self.infer_graph = tf.Graph()\n",
    "        with self.infer_graph.as_default():\n",
    "            self.infer_in_seq = tf.placeholder(tf.int32, shape=[1, None])\n",
    "            self.infer_in_seq_len = tf.placeholder(tf.int32, shape=[1])\n",
    "            self.infer_output = seq2seq(self.infer_in_seq,\n",
    "                                        self.infer_in_seq_len, None, None,\n",
    "                                        len(self.infer_vocabs), self.num_units,\n",
    "                                        self.layers, self.dropout)\n",
    "            \n",
    "            self.infer_saver = tf.train.Saver()\n",
    "        \n",
    "        self.infer_session = tf.Session(\n",
    "            graph=self.infer_graph, config=self.gpu_session_config())\n",
    "\n",
    "    def train(self, epochs, start=0):\n",
    "        if not self.init_train:\n",
    "            raise Exception('Train graph is not inited!')\n",
    "        \n",
    "        with self.train_graph.as_default():\n",
    "            if path.isfile(self.model_file + '.meta') and self.restore_model:\n",
    "                print(\"Reloading model file before training.\")\n",
    "                self.train_saver.restore(self.train_session, self.model_file)\n",
    "            else:\n",
    "                self.train_session.run(self.train_init)\n",
    "            \n",
    "            total_loss = 0\n",
    "            for step in range(start, epochs):\n",
    "                data = next(self.train_data) # yeild\n",
    "                in_seq = data['in_seq']\n",
    "                in_seq_len = data['in_seq_len']\n",
    "                target_seq = data['target_seq']\n",
    "                target_seq_len = data['target_seq_len']\n",
    "                \n",
    "                output, loss, train, summary = self.train_session.run(\n",
    "                    [\n",
    "                        self.train_output, \n",
    "                        self.loss, \n",
    "                        self.train_op,\n",
    "                        self.train_summary\n",
    "                    ],\n",
    "                    feed_dict={\n",
    "                        self.train_in_seq: in_seq,\n",
    "                        self.train_in_seq_len: in_seq_len,\n",
    "                        self.train_target_seq: target_seq,\n",
    "                        self.train_target_seq_len: target_seq_len\n",
    "                    })\n",
    "                \n",
    "                total_loss += loss\n",
    "                \n",
    "                self.log_writter.add_summary(summary, step)\n",
    "                \n",
    "                if step % self.save_step == 0:\n",
    "                    self.train_saver.save(self.train_session, self.model_file)\n",
    "                    print((\"Saving model. Step: %d, loss: %f\" %\n",
    "                           (step, total_loss / self.save_step)))\n",
    "                    # print sample output\n",
    "                    sid = random.randint(0, self.batch_size - 1)\n",
    "                    input_text = decode_text(in_seq[sid],\n",
    "                                                    self.eval_reader.vocabs)\n",
    "                    output_text = decode_text(output[sid],\n",
    "                                                     self.train_reader.vocabs)\n",
    "                    target_text = decode_text(\n",
    "                        target_seq[sid],\n",
    "                        self.train_reader.vocabs).split(' ')[1:]\n",
    "                    target_text = ' '.join(target_text)\n",
    "                    print('******************************')\n",
    "                    print(('src: ' + input_text))\n",
    "                    print(('output: ' + output_text))\n",
    "                    print(('target: ' + target_text))\n",
    "                    \n",
    "                if step % self.eval_step == 0:\n",
    "                    bleu_score = self.eval(step)\n",
    "                    print((\"Evaluate model. Step: %d, score: %f, loss: %f\" %\n",
    "                           (step, bleu_score, total_loss / self.save_step)))\n",
    "                    eval_summary = tf.Summary(value=[\n",
    "                        tf.Summary.Value(tag='bleu', simple_value=bleu_score)\n",
    "                    ])\n",
    "                    self.log_writter.add_summary(eval_summary, step)\n",
    "                \n",
    "                if step % self.save_step == 0:\n",
    "                    total_loss = 0\n",
    "\n",
    "    def eval(self, train_step):\n",
    "        '''测试函数，bleu_score'''\n",
    "        with self.eval_graph.as_default():\n",
    "            self.eval_saver.restore(self.eval_session, self.model_file)\n",
    "            bleu_score = 0\n",
    "            target_results = []\n",
    "            output_results = []\n",
    "            for step in range(0, self.eval_reader.data_size):\n",
    "                data = next(self.eval_data)\n",
    "                in_seq = data['in_seq']\n",
    "                in_seq_len = data['in_seq_len']\n",
    "                target_seq = data['target_seq']\n",
    "                target_seq_len = data['target_seq_len']\n",
    "                outputs = self.eval_session.run(\n",
    "                    self.eval_output,\n",
    "                    feed_dict={\n",
    "                        self.eval_in_seq: in_seq,\n",
    "                        self.eval_in_seq_len: in_seq_len\n",
    "                    })\n",
    "                \n",
    "                for i in range(len(outputs)):\n",
    "                    output = outputs[i]\n",
    "                    target = target_seq[i]\n",
    "                    output_text = decode_text(\n",
    "                        output, self.eval_reader.vocabs).split(' ')\n",
    "                    target_text = decode_text(\n",
    "                        target[1:], self.eval_reader.vocabs).split(' ')\n",
    "                    \n",
    "                    prob = int(\n",
    "                        self.eval_reader.data_size * self.batch_size / 10)\n",
    "                    \n",
    "                    target_results.append([target_text])\n",
    "                    output_results.append(output_text)\n",
    "                    # 随机输出结果\n",
    "                    if random.randint(1, prob) == 1:\n",
    "                        print('====================')\n",
    "                        input_text = decode_text(\n",
    "                            in_seq[i], self.eval_reader.vocabs)\n",
    "                        print(('src:' + input_text))\n",
    "                        print(('output: ' + ' '.join(output_text)))\n",
    "                        print(('target: ' + ' '.join(target_text)))\n",
    "                        \n",
    "            return compute_bleu(target_results, output_results)[0] * 100\n",
    "\n",
    "    def reload_infer_model(self):\n",
    "        # 重新加载推理图\n",
    "        with self.infer_graph.as_default():\n",
    "            self.infer_saver.restore(self.infer_session, self.model_file)\n",
    "\n",
    "    def infer(self, text):\n",
    "        if not self.init_infer:\n",
    "            raise Exception('Infer graph is not inited!')\n",
    "            \n",
    "        with self.infer_graph.as_default():\n",
    "            in_seq = encode_text(\n",
    "                text.split(' ') + [\n",
    "                    '</s>',\n",
    "                ], self.infer_vocab_indices)\n",
    "            in_seq_len = len(in_seq)\n",
    "            \n",
    "            outputs = self.infer_session.run(\n",
    "                self.infer_output,\n",
    "                feed_dict={\n",
    "                    self.infer_in_seq: [in_seq],\n",
    "                    self.infer_in_seq_len: [in_seq_len]\n",
    "                })\n",
    "            output = outputs[0]\n",
    "            output_text = decode_text(output, self.infer_vocabs)\n",
    "            return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model. Step: 0, loss: 1.017375\n",
      "******************************\n",
      "src: 龙 根 在 抱 ， 基 业 于 胸 ， 发 展 进 行 时 ， 须 知 岁 月 无 期 、 资 源 有 限\n",
      "output: 牿 帮 帮 酼 酼 酼 酼 酼 邀 酼 灣 灣 灣 灣 灣 灣 灣 灣 灣 灣 灣 灣 灣 灣 帮 帮 帮 帮\n",
      "target: 红 线 铭 心 ， 蓝 图 入 梦 ， 和 谐 规 划 处 ， 尤 记 尺 疆 易 失 、 寸 土 难 还\n",
      "INFO:tensorflow:Restoring parameters from ./models/output_couplet\\model.ckpl\n",
      "====================\n",
      "src:千 里 青 青 草 ， 碧 色 连 天 ， 无 边 春 色 共 谁 看\n",
      "output: \n",
      "target: 一 池 淡 淡 荷 ， 清 香 扑 面 ， 几 度 花 香 伴 月 眠\n",
      "====================\n",
      "src:青 山 随 我 隐\n",
      "output: \n",
      "target: 碧 水 为 君 酬\n",
      "====================\n",
      "src:治 水 禹 王 功 ， 看 龙 伏 波 平 ， 赫 赫 千 秋 歌 大 业\n",
      "output: \n",
      "target: 富 民 政 策 好 ， 喜 鱼 香 稻 熟 ， 熙 熙 百 姓 乐 丰 年\n",
      "====================\n",
      "src:苦 楚\n",
      "output: \n",
      "target: 艰 难\n",
      "====================\n",
      "src:雨 前 茶 绿 宜 烹 雪\n",
      "output: \n",
      "target: 宴 罢 酒 高 莫 驾 车\n",
      "====================\n",
      "src:以 善 结 禅 缘 ， 诚 通 佛 性 ， 人 和 兼 地 利\n",
      "output: \n",
      "target: 任 山 扬 龙 首 ， 水 唱 凤 歌 ， 雨 顺 更 风 调\n",
      "====================\n",
      "src:万 里 香 风 菩 萨 道\n",
      "output: \n",
      "target: 一 声 梵 唱 华 严 经\n",
      "====================\n",
      "src:释 怀 心 见 佛\n",
      "output: \n",
      "target: 难 道 不 如 人\n",
      "====================\n",
      "src:孝 容 海 岳 ， 古 邑 人 文 尤 焕 彩\n",
      "output: \n",
      "target: 义 薄 云 天 ， 新 时 山 水 更 添 娇\n",
      "Evaluate model. Step: 0, score: 0.000000, loss: 1.017375\n",
      "Saving model. Step: 100, loss: 71.098539\n",
      "******************************\n",
      "src: 八 骏 呈 祥 功 卓 著\n",
      "output: 风 风 ， ， ， ，\n",
      "target: 三 羊 贺 岁 事 兴 隆\n",
      "Saving model. Step: 200, loss: 64.311578\n",
      "******************************\n",
      "src: 诗 魂 歌 魄 留 青 史\n",
      "output: 一 风 风 ， ， 风\n",
      "target: 义 胆 忠 心 系 白 沙\n",
      "Saving model. Step: 300, loss: 63.526870\n",
      "******************************\n",
      "src: 高 山 徒 仰 止\n",
      "output: 月 风 月 月 人\n",
      "target: 急 景 易 蹉 跎\n",
      "Saving model. Step: 400, loss: 63.195465\n",
      "******************************\n",
      "src: 板 斜 尿 流 急\n",
      "output: 一 月 月 月 风\n",
      "target: 坑 深 粪 落 迟\n",
      "Saving model. Step: 500, loss: 62.519723\n",
      "******************************\n",
      "src: 腾 云 驾 雾 ， 英 雄 再 踏 飞 天 路\n",
      "output: 一 风 一 ， ， ， ， ， ， 万 新 天\n",
      "target: 揽 月 追 星 ， 勇 士 又 书 动 地 诗\n",
      "Saving model. Step: 600, loss: 61.954388\n",
      "******************************\n",
      "src: 舟 行 江 心 处 ， 满 江 红 ， 橹 速 帆 快\n",
      "output: 一 风 一 ， ， ， ， ， ， ， ， 月 春 香\n",
      "target: 云 栖 竹 径 时 ， 丝 竹 响 ， 笛 清 箫 和\n",
      "Saving model. Step: 700, loss: 61.306664\n",
      "******************************\n",
      "src: 青 花 釉 里 红 梅 绽\n",
      "output: 风 水 花 花 月 月 香\n",
      "target: 紫 陌 尘 中 白 马 飞\n",
      "Saving model. Step: 800, loss: 61.774655\n",
      "******************************\n",
      "src: 与 世 无 争 ， 老 叟 寒 江 独 钓\n",
      "output: 人 心 无 ， ， ， ， ， 水 水 风\n",
      "target: 消 愁 有 术 ， 闲 人 草 舍 自 斟\n",
      "Saving model. Step: 900, loss: 60.662572\n",
      "******************************\n",
      "src: 春 山 问 道 东 风 指\n",
      "output: 风 风 风 风 月 月 情\n",
      "target: 碧 浪 飞 天 明 月 悬\n",
      "Saving model. Step: 1000, loss: 60.202890\n",
      "******************************\n",
      "src: 蓝 茜 纷 呈 ， 花 尤 好 引 竹 而 开 ， 自 率 一 分 清 气\n",
      "output: 春 风 不 ， ， ， ， ， ， ， ， ， ， ， ， 一 一 千 春\n",
      "target: 青 春 易 逝 ， 人 亦 当 闻 鸡 则 起 ， 不 辜 半 点 流 光\n",
      "INFO:tensorflow:Restoring parameters from ./models/output_couplet\\model.ckpl\n",
      "====================\n",
      "src:风 弦 未 拨 心 先 乱\n",
      "output: 月 月 无 风 不 月 人\n",
      "target: 夜 幕 已 沉 梦 更 闲\n",
      "====================\n",
      "src:跃 马 挥 戈 ， 反 腐 倡 廉 传 捷 报\n",
      "output: 文 风 ， ， ， ， ， ， ， 大 新 风\n",
      "target: 腾 羊 舞 剑 ， 兴 邦 治 国 奏 和 弦\n",
      "====================\n",
      "src:拓 出 潭 城 一 片 天 ， 计 生 播 福\n",
      "output: 一 风 万 ， ， ， ， ， ， 一 新 春\n",
      "target: 裁 来 林 海 千 重 翠 ， 国 策 铺 春\n",
      "====================\n",
      "src:名 花 无 好 主\n",
      "output: 不 月 不 无 人\n",
      "target: 小 草 有 高 怀\n",
      "====================\n",
      "src:东 南 再 造 ， 砥 柱 勋 高 ， 与 斯 民 食 德 饮 和 ， 上 慰 宸 衷 隆 眷 顾\n",
      "output: 文 文 ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， ， 不 为 人\n",
      "target: 中 外 同 文 ， 长 城 望 重 ， 痛 此 日 鞠 躬 尽 瘁 ， 永 怀 臣 品 励 精 诚\n",
      "====================\n",
      "src:参 差 近 水 楼 台 ， 风 月 已 供 无 尽 藏\n",
      "output: 不 风 不 ， ， ， ， ， ， ， ， 不 无 人\n",
      "target: 中 有 吹 箫 俦 侣 ， 溪 山 好 处 便 为 家\n",
      "====================\n",
      "src:求 荣 卖 友 情 难 继\n",
      "output: 不 德 无 ， ， 为 人\n",
      "target: 弃 政 眠 松 品 自 高\n",
      "====================\n",
      "src:满 头 雾 水\n",
      "output: 一 月 月 香\n",
      "target: 过 眼 云 烟\n",
      "====================\n",
      "src:云 藏 空 岫 厌 天 阔\n",
      "output: 月 月 春 风 月 月 风\n",
      "target: 霞 落 清 波 喜 海 平\n",
      "====================\n",
      "src:鼎 钟 上 古 字\n",
      "output: 春 水 月 风 风\n",
      "target: 金 石 大 家 文\n",
      "Evaluate model. Step: 1000, score: 0.000000, loss: 60.202890\n",
      "Saving model. Step: 1100, loss: 59.332437\n",
      "******************************\n",
      "src: 龙 帝 名 联 辉 日 月\n",
      "output: 风 风 春 展 展 春 风\n",
      "target: 蛇 皇 伟 业 耀 乾 坤\n",
      "Saving model. Step: 1200, loss: 58.830855\n",
      "******************************\n",
      "src: 雪 瀑 贺 新 居 ， 蟠 龙 喜 挂 水 晶 帘 ， 百 里 画 廊 收 眼 底\n",
      "output: 春 风 春 水 ， ， 一 里 ， ， ， ， ， ， ， ， 一 里 里 春 春\n",
      "target: 清 江 飘 碧 带 ， 宝 邸 平 添 青 玉 镜 ， 四 时 春 色 映 心 头\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-88f0a543f768>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-7d46c59fe5b9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, start)\u001b[0m\n\u001b[0;32m    182\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_in_seq_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0min_seq_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_target_seq\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_target_seq_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtarget_seq_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    185\u001b[0m                     })\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m = Model(\n",
    "        './couplet/train/in.txt',\n",
    "        './couplet/train/out.txt',\n",
    "        './couplet/test/in.txt',\n",
    "        './couplet/test/out.txt',\n",
    "        './couplet/vocabs',\n",
    "        num_units=256,\n",
    "        layers=4,\n",
    "        dropout=0.2,\n",
    "        batch_size=32,\n",
    "        learning_rate=0.001,\n",
    "        output_dir='./models/output_couplet',\n",
    "        restore_model=False\n",
    ")\n",
    "\n",
    "m.train(5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
