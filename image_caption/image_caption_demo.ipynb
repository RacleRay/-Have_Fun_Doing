{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# generate_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:14:41.473651Z",
     "start_time": "2019-07-03T13:14:40.144057Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "{2: 14,\n",
      " 3: 52,\n",
      " 4: 297,\n",
      " 5: 1109,\n",
      " 6: 3593,\n",
      " 7: 7895,\n",
      " 8: 11070,\n",
      " 9: 13165,\n",
      " 10: 14821,\n",
      " 11: 15427,\n",
      " 12: 14481,\n",
      " 13: 12919,\n",
      " 14: 11394,\n",
      " 15: 9952,\n",
      " 16: 8121,\n",
      " 17: 6628,\n",
      " 18: 5352,\n",
      " 19: 4353,\n",
      " 20: 3510,\n",
      " 21: 2763,\n",
      " 22: 2363,\n",
      " 23: 1779,\n",
      " 24: 1388,\n",
      " 25: 1169,\n",
      " 26: 1004,\n",
      " 27: 778,\n",
      " 28: 647,\n",
      " 29: 496,\n",
      " 30: 397,\n",
      " 31: 362,\n",
      " 32: 271,\n",
      " 33: 228,\n",
      " 34: 165,\n",
      " 35: 165,\n",
      " 36: 132,\n",
      " 37: 93,\n",
      " 38: 102,\n",
      " 39: 61,\n",
      " 40: 61,\n",
      " 41: 54,\n",
      " 42: 39,\n",
      " 43: 40,\n",
      " 44: 26,\n",
      " 45: 23,\n",
      " 46: 21,\n",
      " 47: 12,\n",
      " 48: 8,\n",
      " 49: 17,\n",
      " 50: 7,\n",
      " 51: 18,\n",
      " 52: 13,\n",
      " 53: 8,\n",
      " 54: 4,\n",
      " 55: 9,\n",
      " 56: 6,\n",
      " 57: 5,\n",
      " 58: 2,\n",
      " 59: 5,\n",
      " 60: 2,\n",
      " 61: 1,\n",
      " 62: 1,\n",
      " 63: 1,\n",
      " 64: 2,\n",
      " 65: 1,\n",
      " 66: 1,\n",
      " 67: 2,\n",
      " 68: 1,\n",
      " 69: 2,\n",
      " 70: 1,\n",
      " 71: 1,\n",
      " 72: 1,\n",
      " 73: 1,\n",
      " 75: 1,\n",
      " 79: 1,\n",
      " 82: 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "\n",
    "input_description_file = \"./data/image_caption_data/results_20130124.token\"\n",
    "output_vocab_file = \"./data/image_caption_data/vocab.txt\"\n",
    "\n",
    "\n",
    "def count_vocab(input_token_filename):\n",
    "    '''词表信息统计(英文词表)，生成词表'''\n",
    "    with open(input_token_filename, 'r', encoding='UTF-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    max_length_of_sentences = 0\n",
    "    length_dict = {}   # 长度统计\n",
    "    vocab_dict = {}    # 词频统计\n",
    "    \n",
    "    for line in lines:\n",
    "        image_id, description = line.strip('\\n').split('\\t')\n",
    "        words = description.strip(' ').split()\n",
    "        max_length_of_sentences = max(max_length_of_sentences, len(words))\n",
    "        length_dict.setdefault(len(words), 0)\n",
    "        length_dict[len(words)] += 1\n",
    "\n",
    "        for word in words:\n",
    "            vocab_dict.setdefault(word, 0)\n",
    "            vocab_dict[word] += 1\n",
    "    \n",
    "    print(max_length_of_sentences)\n",
    "    # pprint将列表、字典、元组以易读格式打印\n",
    "    pprint.pprint(length_dict)\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = count_vocab(input_description_file)\n",
    "# 最大长度的设定，假如选择82，那么会有太多的无意义padding句子。并且\n",
    "# 选择太小则会有太多的信息损失，做个trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:17:06.484879Z",
     "start_time": "2019-07-03T13:17:06.394090Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 排序\n",
    "sorted_vocab_dict = sorted(vocab_dict.items(),\n",
    "                          key=lambda d: d[1], reverse=True)\n",
    "with open(output_vocab_file, 'w', encoding='UTF-8') as f:\n",
    "    f.write(\"<UNK>\\t100000\\n\")  # 未在词表中\n",
    "    for item in sorted_vocab_dict:\n",
    "        f.write('%s\\t%d\\n' % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:17:26.749310Z",
     "start_time": "2019-07-03T13:17:14.421669Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:18:35.142896Z",
     "start_time": "2019-07-03T13:18:34.335185Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 预训练好的模型文件\n",
    "model_file = \"./data/image_caption_data/checkpoint_inception_v3/inception_v3_graph_def.pb\"\n",
    "# 图像描述文件\n",
    "input_description_file = \"./data/image_caption_data/results_20130124.token\"\n",
    "# 图片\n",
    "input_img_dir = \"./data/image_caption_data/flickr30k_images/\"\n",
    "# 特征提取输出（比较耗时的操作，已计算好）\n",
    "output_folder = \"./data/image_caption_data/feature_extraction_inception_v3\"\n",
    "\n",
    "# 将所有图片按批量提取并输出，成为多个小文件，方便读写\n",
    "batch_size = 1000\n",
    "\n",
    "# 分布式环境中，os不能工作，gfile是可以工作的（也最好统一）\n",
    "if not gfile.Exists(output_folder):\n",
    "    gfile.MakeDirs(output_folder)\n",
    "\n",
    "\n",
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses token file.\"\"\"\n",
    "    # 图片名到描述\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "\n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "all_img_names = img_name_to_tokens.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:19:06.618652Z",
     "start_time": "2019-07-03T13:19:06.611678Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg',\n",
      " '1000523639.jpg',\n",
      " '1000919630.jpg',\n",
      " '10010052.jpg',\n",
      " '1001465944.jpg',\n",
      " '1001545525.jpg']\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"num of all images: %d\" % len(all_img_names))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:10])\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:20:22.718073Z",
     "start_time": "2019-07-03T13:20:20.145939Z"
    },
    "hidden": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "num_batches = int(len(all_img_names) /  batch_size)\n",
    "if len(all_img_names) % batch_size != 0:\n",
    "    num_batches += 1\n",
    "\n",
    "\n",
    "def load_pretrained_inception_v3(model_file):\n",
    "    '''计算图和权重文件gfile导入'''\n",
    "    with gfile.FastGFile(model_file, \"rb\") as f:\n",
    "        # 新建空的计算图\n",
    "        graph_def = tf.GraphDef()\n",
    "        # 读取预训练计算图\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        # 导入默认计算图：打开一个session就会打开一个默认graph\n",
    "        tf.import_graph_def(graph_def, name=\"\")\n",
    "\n",
    "        \n",
    "load_pretrained_inception_v3(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T13:24:37.735392Z",
     "start_time": "2019-07-03T13:24:37.731402Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # 使用倒数第二层的输出\n",
    "    second_to_last_tensor = sess.graph.get_tensor_by_name(\"pool_3:0\")\n",
    "    for i in range(num_batches):\n",
    "        batch_img_names = list(all_img_names)[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_features = []\n",
    "        for img_name in batch_img_names:\n",
    "            img_path = os.path.join(input_img_dir, img_name)\n",
    "            logging.info(\"processing img %s\" % img_name)\n",
    "            if not gfile.Exists(img_path):\n",
    "                raise Exception(\"%s doesn't exists\" % img_path)\n",
    "            img_data = gfile.FastGFile(img_path, \"rb\").read()\n",
    "            feature_vector = sess.run(\n",
    "                second_to_last_tensor,\n",
    "                feed_dict={\"DecodeJpeg/contents:0\": img_data})\n",
    "            batch_features.append(feature_vector)\n",
    "        # 转换成矩阵文件\n",
    "        batch_features = np.vstack(batch_features)\n",
    "        output_filename = os.path.join(output_folder,\n",
    "                                       \"image_features-%d.pickle\" % i)\n",
    "        logging.info(\"writing to file %s\" % output_filename)\n",
    "        with gfile.GFile(output_filename, 'w') as f:\n",
    "            # 一一对应的压缩\n",
    "            pickle.dump((batch_img_names, batch_features), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image_caption_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:48:51.853720Z",
     "start_time": "2019-07-03T16:48:51.848745Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tfe.enable_eager_execution()\n",
    "\n",
    "# 第二步提取的图片特征\n",
    "input_img_feature_dir = \"./data/image_caption_data/feature_extraction_inception_v3\"\n",
    "# 图像描述文件\n",
    "input_description_file = \"./data/image_caption_data/results_20130124.token\"\n",
    "# 输出\n",
    "output_dir = \"./data/image_caption_data/local_run\"\n",
    "# 词表\n",
    "input_vocab_file = \"./data/image_caption_data/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:49:06.832905Z",
     "start_time": "2019-07-03T16:49:06.825919Z"
    }
   },
   "outputs": [],
   "source": [
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold=3,\n",
    "        num_embedding_nodes=32,\n",
    "        num_timesteps=10,\n",
    "        num_lstm_nodes=[64, 64],\n",
    "        num_lstm_layers=2,\n",
    "        num_fc_nodes=32,\n",
    "        batch_size=50,\n",
    "        cell_type='lstm',\n",
    "        clip_lstm_grads=1.0,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=0.8,\n",
    "        log_frequent=100,\n",
    "        save_frequent=1000,\n",
    "    )\n",
    "\n",
    "# 超参数\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词表处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:49:08.651630Z",
     "start_time": "2019-07-03T16:49:08.642655Z"
    }
   },
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"词处理类，保存词和ids的mapping\"\"\"\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word.get(cur_id, '<UNK>')\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        'return: list, ids'\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids\n",
    "\n",
    "    def decode(self, sentence_id):\n",
    "        'return: string, words'\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:49:11.394933Z",
     "start_time": "2019-07-03T16:49:11.388950Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses token file.\n",
    "    {img_name: [description]}\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts tokens of each description of imgs to id. \"\"\"\n",
    "    img_name_to_token_ids = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_token_ids.setdefault(img_name, [])\n",
    "        descriptions = img_name_to_tokens[img_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_token_ids[img_name].append(token_ids)\n",
    "    return img_name_to_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:49:14.145776Z",
     "start_time": "2019-07-03T16:49:11.967434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 10875\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg',\n",
      " '1000523639.jpg',\n",
      " '1000919630.jpg',\n",
      " '10010052.jpg',\n",
      " '1001465944.jpg',\n",
      " '1001545525.jpg']\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg',\n",
      " '1000523639.jpg',\n",
      " '1000919630.jpg',\n",
      " '10010052.jpg',\n",
      " '1001465944.jpg',\n",
      " '1001545525.jpg']\n",
      "[[3, 9, 4, 132, 8, 3532, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "    \n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_token_ids = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:10])\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token_ids))\n",
    "pprint.pprint(list(img_name_to_token_ids.keys())[0:10])\n",
    "pprint.pprint(img_name_to_token_ids['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练batch数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:49:16.379602Z",
     "start_time": "2019-07-03T16:49:16.365631Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageCaptionData(object):\n",
    "    def __init__(self,\n",
    "                 img_name_to_token_ids,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "\n",
    "        self._img_name_to_token_ids = img_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps  # 目标解码长度\n",
    "        self._indicator = 0\n",
    "        self._deterministic = deterministic  # 是否随机打乱\n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        self._load_img_feature_pickle()\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "    def _load_img_feature_pickle(self):\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = pickle.load(f, encoding='iso-8859-1')\n",
    "                self._img_feature_filenames += filenames\n",
    "                self._img_feature_data.append(features)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        origin_shape = self._img_feature_data.shape\n",
    "        # 每个图的特征转换为一个序列特征\n",
    "        self._img_feature_data = np.reshape(\n",
    "            self._img_feature_data, (origin_shape[0], origin_shape[3]))\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "\n",
    "    def _img_desc(self, filenames):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._img_name_to_token_ids[filename]\n",
    "            # chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids = token_ids_set[0]\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "            # 控制训练数据长度\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        batch_img_names = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_img_names)\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:49:26.751875Z",
     "start_time": "2019-07-03T16:49:18.318224Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/image_caption_data/feature_extraction_inception_v3\\\\image_features-0.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-1.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-10.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-11.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-12.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-13.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-14.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-15.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-16.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-17.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-18.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-19.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-2.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-20.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-21.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-22.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-23.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-24.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-25.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-26.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-27.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-28.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-29.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-3.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-30.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-31.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-4.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-5.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-6.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-7.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-8.pickle',\n",
      " './data/image_caption_data/feature_extraction_inception_v3\\\\image_features-9.pickle']\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-0.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-1.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-10.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-11.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-12.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-13.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-14.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-15.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-16.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-17.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-18.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-19.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-2.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-20.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-21.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-22.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-23.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-24.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-25.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-26.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-27.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-28.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-29.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-3.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-30.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-31.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-4.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-5.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-6.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-7.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-8.pickle\n",
      "INFO:tensorflow:loading ./data/image_caption_data/feature_extraction_inception_v3\\image_features-9.pickle\n",
      "(31783, 2048)\n",
      "(31783,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[0.8736678 , 0.11291973, 0.3496377 , ..., 0.16903086, 0.05070304,\n",
      "        0.5911637 ],\n",
      "       [0.26011327, 0.20115225, 0.30232993, ..., 1.1717898 , 0.00239144,\n",
      "        0.03500613],\n",
      "       [0.27612504, 0.23093665, 1.1244775 , ..., 0.879735  , 0.8691849 ,\n",
      "        0.3917613 ],\n",
      "       [0.11328986, 0.74356484, 0.15022026, ..., 2.1486602 , 0.3694371 ,\n",
      "        0.21831551],\n",
      "       [0.9466319 , 0.47574863, 0.28378642, ..., 0.5749301 , 0.335031  ,\n",
      "        0.9551453 ]], dtype=float32)\n",
      "array([[   3,  673,   30,   18,    1,   91,   21,    7,  945,  123],\n",
      "       [   3,   73,   39,   10,  238, 1857,   23,   19,  466,  140],\n",
      "       [   3,    9,   96,    4,   25,  236,   15,    5,  351,   12],\n",
      "       [   3,   20,   33,  637,   62, 2962,    0,  709,   62,    5],\n",
      "       [  59, 6761,    4,   28,  128,    7,   20,  260,  534,    6]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "array(['3329796982.jpg', '2386366490.jpg', '4809325086.jpg',\n",
      "       '3264337159.jpg', '4887197825.jpg'], dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "caption_data = ImageCaptionData(img_name_to_token_ids, input_img_feature_dir, hps.num_timesteps, vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:49:29.844906Z",
     "start_time": "2019-07-03T16:49:28.643711Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-7d7ff1794ec7>:46: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logits/bias:0\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/kernel:0_grad is illegal; using image_feature_embed/dense/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/bias:0_grad is illegal; using image_feature_embed/dense/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/kernel:0_grad is illegal; using fc/logits/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/bias:0_grad is illegal; using fc/logits/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "# from AdaBound import AdaBoundOptimizer\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "        \n",
    "def dropout(cell, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    img_feature  = tf.placeholder(tf.float32, (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    \n",
    "    # prediction process:\n",
    "    #    sentence: [a,,c,f,...]\n",
    "    #    input: [img, a, c, ...]\n",
    "    #    img_feature: [0.12, 0.123, ...]\n",
    "    #    predict: img_feature -> embedding_img -> lstm -> a\n",
    "    #             a -> embedding_word -> lstm -> b \n",
    "    \n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        # 取到倒数第二个token，因为预测第一步是img输入\n",
    "        # [batchsize, num_timesteps-1, num_embedding_nodes]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps-1])\n",
    "\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=img_feature_embed_init):\n",
    "        # 将img feature作为第一步输入\n",
    "        # [batchsize, num_embedding_nodes]\n",
    "        embed_img = tf.layers.dense(img_feature, hps.num_embedding_nodes)\n",
    "        # [batchsize, 1, num_embedding_nodes]\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        # [batchsize, num_timesteps, num_embedding_nodes]\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)\n",
    "\n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, hps.num_lstm_node[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                           embed_inputs,\n",
    "                                           initial_state=initial_state)\n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs, [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_outputs_2d, hps.num_fc_nodes, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_dropout, vocab_size, name='logits')\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])\n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        \n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=sentence_flatten)\n",
    "        weighted_softmax_loss = tf.multiply(softmax_loss,\n",
    "                                            tf.cast(mask_flatten, tf.float32))\n",
    "        \n",
    "        prediction = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "        correct_prediction = tf.equal(prediction, sentence_flatten)\n",
    "        correct_prediction_with_mask = tf.multiply(\n",
    "            tf.cast(correct_prediction, tf.float32),\n",
    "            mask_flatten)\n",
    "        \n",
    "        # 参考指标\n",
    "        accuracy = tf.reduce_sum(correct_prediction_with_mask) / mask_sum\n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / mask_sum\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "            (loss, accuracy, train_op),\n",
    "            global_step)\n",
    "\n",
    "placeholders, metrics, global_step = get_train_model(hps, vocab_size, img_feature_dim)\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:56:04.338334Z",
     "start_time": "2019-07-03T16:49:31.883554Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   100, loss: 6.098, accuracy: 0.102\n",
      "INFO:tensorflow:Step:   200, loss: 5.526, accuracy: 0.138\n",
      "INFO:tensorflow:Step:   300, loss: 5.283, accuracy: 0.174\n",
      "INFO:tensorflow:Step:   400, loss: 4.881, accuracy: 0.211\n",
      "INFO:tensorflow:Step:   500, loss: 4.879, accuracy: 0.190\n",
      "INFO:tensorflow:Step:   600, loss: 4.888, accuracy: 0.218\n",
      "INFO:tensorflow:Step:   700, loss: 4.454, accuracy: 0.272\n",
      "INFO:tensorflow:Step:   800, loss: 4.789, accuracy: 0.234\n",
      "INFO:tensorflow:Step:   900, loss: 4.708, accuracy: 0.218\n",
      "INFO:tensorflow:Step:  1000, loss: 4.764, accuracy: 0.216\n",
      "INFO:tensorflow:Step: 1000, image caption model saved\n",
      "INFO:tensorflow:Step:  1100, loss: 4.440, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1200, loss: 4.557, accuracy: 0.226\n",
      "INFO:tensorflow:Step:  1300, loss: 4.277, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  1400, loss: 4.754, accuracy: 0.218\n",
      "INFO:tensorflow:Step:  1500, loss: 4.429, accuracy: 0.230\n",
      "INFO:tensorflow:Step:  1600, loss: 4.411, accuracy: 0.258\n",
      "INFO:tensorflow:Step:  1700, loss: 4.596, accuracy: 0.222\n",
      "INFO:tensorflow:Step:  1800, loss: 4.113, accuracy: 0.270\n",
      "INFO:tensorflow:Step:  1900, loss: 3.987, accuracy: 0.278\n",
      "INFO:tensorflow:Step:  2000, loss: 4.319, accuracy: 0.244\n",
      "INFO:tensorflow:Step: 2000, image caption model saved\n",
      "INFO:tensorflow:Step:  2100, loss: 4.221, accuracy: 0.256\n",
      "INFO:tensorflow:Step:  2200, loss: 4.194, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  2300, loss: 4.041, accuracy: 0.272\n",
      "INFO:tensorflow:Step:  2400, loss: 4.052, accuracy: 0.292\n",
      "INFO:tensorflow:Step:  2500, loss: 4.227, accuracy: 0.256\n",
      "INFO:tensorflow:Step:  2600, loss: 3.836, accuracy: 0.298\n",
      "INFO:tensorflow:Step:  2700, loss: 4.411, accuracy: 0.208\n",
      "INFO:tensorflow:Step:  2800, loss: 3.945, accuracy: 0.324\n",
      "INFO:tensorflow:Step:  2900, loss: 3.993, accuracy: 0.280\n",
      "INFO:tensorflow:Step:  3000, loss: 3.898, accuracy: 0.280\n",
      "INFO:tensorflow:Step: 3000, image caption model saved\n",
      "INFO:tensorflow:Step:  3100, loss: 3.905, accuracy: 0.286\n",
      "INFO:tensorflow:Step:  3200, loss: 4.003, accuracy: 0.280\n",
      "INFO:tensorflow:Step:  3300, loss: 3.832, accuracy: 0.282\n",
      "INFO:tensorflow:Step:  3400, loss: 4.021, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  3500, loss: 3.959, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  3600, loss: 3.809, accuracy: 0.274\n",
      "INFO:tensorflow:Step:  3700, loss: 4.339, accuracy: 0.236\n",
      "INFO:tensorflow:Step:  3800, loss: 3.892, accuracy: 0.288\n",
      "INFO:tensorflow:Step:  3900, loss: 4.082, accuracy: 0.248\n",
      "INFO:tensorflow:Step:  4000, loss: 3.927, accuracy: 0.280\n",
      "INFO:tensorflow:Step: 4000, image caption model saved\n",
      "INFO:tensorflow:Step:  4100, loss: 4.029, accuracy: 0.282\n",
      "INFO:tensorflow:Step:  4200, loss: 3.870, accuracy: 0.282\n",
      "INFO:tensorflow:Step:  4300, loss: 3.790, accuracy: 0.308\n",
      "INFO:tensorflow:Step:  4400, loss: 3.741, accuracy: 0.304\n",
      "INFO:tensorflow:Step:  4500, loss: 3.290, accuracy: 0.330\n",
      "INFO:tensorflow:Step:  4600, loss: 4.088, accuracy: 0.262\n",
      "INFO:tensorflow:Step:  4700, loss: 4.278, accuracy: 0.262\n",
      "INFO:tensorflow:Step:  4800, loss: 3.682, accuracy: 0.324\n",
      "INFO:tensorflow:Step:  4900, loss: 3.995, accuracy: 0.260\n",
      "INFO:tensorflow:Step:  5000, loss: 4.013, accuracy: 0.290\n",
      "INFO:tensorflow:Step: 5000, image caption model saved\n",
      "INFO:tensorflow:Step:  5100, loss: 4.048, accuracy: 0.268\n",
      "INFO:tensorflow:Step:  5200, loss: 3.890, accuracy: 0.278\n",
      "INFO:tensorflow:Step:  5300, loss: 3.487, accuracy: 0.332\n",
      "INFO:tensorflow:Step:  5400, loss: 3.984, accuracy: 0.282\n",
      "INFO:tensorflow:Step:  5500, loss: 4.135, accuracy: 0.270\n",
      "INFO:tensorflow:Step:  5600, loss: 3.988, accuracy: 0.260\n",
      "INFO:tensorflow:Step:  5700, loss: 3.963, accuracy: 0.270\n",
      "INFO:tensorflow:Step:  5800, loss: 3.882, accuracy: 0.298\n",
      "INFO:tensorflow:Step:  5900, loss: 3.953, accuracy: 0.302\n",
      "INFO:tensorflow:Step:  6000, loss: 3.612, accuracy: 0.342\n",
      "INFO:tensorflow:Step: 6000, image caption model saved\n",
      "INFO:tensorflow:Step:  6100, loss: 3.891, accuracy: 0.314\n",
      "INFO:tensorflow:Step:  6200, loss: 4.192, accuracy: 0.274\n",
      "INFO:tensorflow:Step:  6300, loss: 3.666, accuracy: 0.298\n",
      "INFO:tensorflow:Step:  6400, loss: 3.705, accuracy: 0.274\n",
      "INFO:tensorflow:Step:  6500, loss: 3.491, accuracy: 0.310\n",
      "INFO:tensorflow:Step:  6600, loss: 3.883, accuracy: 0.290\n",
      "INFO:tensorflow:Step:  6700, loss: 3.503, accuracy: 0.306\n",
      "INFO:tensorflow:Step:  6800, loss: 3.810, accuracy: 0.304\n",
      "INFO:tensorflow:Step:  6900, loss: 3.579, accuracy: 0.320\n",
      "INFO:tensorflow:Step:  7000, loss: 3.690, accuracy: 0.326\n",
      "INFO:tensorflow:Step: 7000, image caption model saved\n",
      "INFO:tensorflow:Step:  7100, loss: 3.642, accuracy: 0.320\n",
      "INFO:tensorflow:Step:  7200, loss: 3.525, accuracy: 0.306\n",
      "INFO:tensorflow:Step:  7300, loss: 3.680, accuracy: 0.314\n",
      "INFO:tensorflow:Step:  7400, loss: 3.797, accuracy: 0.292\n",
      "INFO:tensorflow:Step:  7500, loss: 3.864, accuracy: 0.276\n",
      "INFO:tensorflow:Step:  7600, loss: 3.878, accuracy: 0.304\n",
      "INFO:tensorflow:Step:  7700, loss: 3.534, accuracy: 0.308\n",
      "INFO:tensorflow:Step:  7800, loss: 3.764, accuracy: 0.286\n",
      "INFO:tensorflow:Step:  7900, loss: 3.974, accuracy: 0.310\n",
      "INFO:tensorflow:Step:  8000, loss: 3.622, accuracy: 0.298\n",
      "INFO:tensorflow:Step: 8000, image caption model saved\n",
      "INFO:tensorflow:Step:  8100, loss: 3.944, accuracy: 0.272\n",
      "INFO:tensorflow:Step:  8200, loss: 3.771, accuracy: 0.298\n",
      "INFO:tensorflow:Step:  8300, loss: 3.602, accuracy: 0.292\n",
      "INFO:tensorflow:Step:  8400, loss: 3.660, accuracy: 0.286\n",
      "INFO:tensorflow:Step:  8500, loss: 3.551, accuracy: 0.336\n",
      "INFO:tensorflow:Step:  8600, loss: 3.501, accuracy: 0.326\n",
      "INFO:tensorflow:Step:  8700, loss: 3.777, accuracy: 0.274\n",
      "INFO:tensorflow:Step:  8800, loss: 3.537, accuracy: 0.320\n",
      "INFO:tensorflow:Step:  8900, loss: 3.341, accuracy: 0.346\n",
      "INFO:tensorflow:Step:  9000, loss: 3.493, accuracy: 0.298\n",
      "INFO:tensorflow:Step: 9000, image caption model saved\n",
      "INFO:tensorflow:Step:  9100, loss: 3.829, accuracy: 0.242\n",
      "INFO:tensorflow:Step:  9200, loss: 3.535, accuracy: 0.302\n",
      "INFO:tensorflow:Step:  9300, loss: 3.561, accuracy: 0.308\n",
      "INFO:tensorflow:Step:  9400, loss: 3.273, accuracy: 0.338\n",
      "INFO:tensorflow:Step:  9500, loss: 3.598, accuracy: 0.302\n",
      "INFO:tensorflow:Step:  9600, loss: 4.020, accuracy: 0.262\n",
      "INFO:tensorflow:Step:  9700, loss: 3.629, accuracy: 0.292\n",
      "INFO:tensorflow:Step:  9800, loss: 3.586, accuracy: 0.294\n",
      "INFO:tensorflow:Step:  9900, loss: 3.757, accuracy: 0.294\n",
      "INFO:tensorflow:Step: 10000, loss: 3.730, accuracy: 0.280\n",
      "INFO:tensorflow:Step: 10000, image caption model saved\n"
     ]
    }
   ],
   "source": [
    "training_steps = 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(training_steps):\n",
    "        batch_img_features, batch_sentence_ids, batch_weights, _ = caption_data.next(hps.batch_size)\n",
    "        input_vals = (batch_img_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "        \n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        \n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        \n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "        \n",
    "        outputs = sess.run(fetches, feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        \n",
    "        if should_log:\n",
    "            summary_str = outputs[4]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, accuracy: %3.3f'\n",
    "                         % (global_step_val, loss_val, accuracy_val))\n",
    "        if should_save:\n",
    "            logging.info(\"Step: %d, image caption model saved\" % (global_step_val))\n",
    "            saver.save(sess, os.path.join(output_dir, \"image_caption\"), global_step=global_step_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:56:06.528920Z",
     "start_time": "2019-07-03T16:56:06.364187Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold=3,\n",
    "        num_embedding_nodes=32,\n",
    "        num_timesteps=10,\n",
    "        num_lstm_nodes=[64, 64],\n",
    "        num_lstm_layers=2,\n",
    "        num_fc_nodes=32,\n",
    "        batch_size=1,\n",
    "        cell_type='lstm',\n",
    "        clip_lstm_grads=1.0,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=0.8,\n",
    "        log_frequent=100,\n",
    "        save_frequent=1000,\n",
    "    )\n",
    "\n",
    "# 超参数\n",
    "hps = get_default_params()\n",
    "\n",
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "def eval_get_embedding_for_img(hps, img_feature_dim):\n",
    "    img_feature  = tf.placeholder(tf.float32, (1, img_feature_dim))\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=img_feature_embed_init):\n",
    "        embed_img = tf.layers.dense(img_feature, hps.num_embedding_nodes)\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        return img_feature, embed_img\n",
    "\n",
    "def eval_embedding_lookup(hps, vocab_size):\n",
    "    word = tf.placeholder(tf.int32, (1, 1))\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_word = tf.nn.embedding_lookup(embeddings, word)\n",
    "    return word, embed_word\n",
    "\n",
    "def eval_lstm_single_step(hps, vocab_size):\n",
    "    embed_input = tf.placeholder(tf.float32, (1, 1, hps.num_embedding_nodes))\n",
    "    num_lstm_layers = []\n",
    "    for i in range(hps.num_lstm_layers):\n",
    "        num_lstm_layers.append(hps.num_lstm_nodes[i])\n",
    "        num_lstm_layers.append(hps.num_lstm_nodes[i])\n",
    "\n",
    "    num_hidden_states = sum(num_lstm_layers)\n",
    "    input_state = tf.placeholder(tf.float32, (1, num_hidden_states))\n",
    "    unpack_init_state = tf.split(input_state, num_lstm_layers, axis=1)\n",
    "    input_tuple_state = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(unpack_init_state):\n",
    "        input_tuple_state.append(\n",
    "            tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                unpack_init_state[i], unpack_init_state[i+1]))\n",
    "        i += 2\n",
    "    input_tuple_state = tuple(input_tuple_state)\n",
    "\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, 1.0)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        decode_cell = BeamSearchDecoder(cell, \n",
    "                                        embedding=)\n",
    "        \n",
    "        rnn_output, output_tuple_state = tf.nn.dynamic_rnn(\n",
    "            cell,\n",
    "            embed_input,\n",
    "            initial_state=input_tuple_state)\n",
    "        output_state = []\n",
    "        for state in output_tuple_state:\n",
    "            output_state.append(state[0])\n",
    "            output_state.append(state[1])\n",
    "        output_state = tf.concat(output_state, axis=1, name=\"output_state\")\n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        rnn_output_2d = tf.reshape(rnn_output, [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_output_2d, hps.num_fc_nodes, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, 1.0)\n",
    "        fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_dropout, vocab_size, name='logits')\n",
    "\n",
    "    return embed_input, rnn_output, logits, input_state, output_state, num_hidden_states\n",
    "\n",
    "\n",
    "img_feature, embed_img = eval_get_embedding_for_img(hps, img_feature_dim)\n",
    "word, embed_word = eval_embedding_lookup(hps, vocab_size)\n",
    "embed_input, rnn_output, logits, input_state, output_state, num_hidden_states = eval_lstm_single_step(hps, vocab_size)\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T16:56:08.572436Z",
     "start_time": "2019-07-03T16:56:08.413971Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:[*] Reading checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from ./data/image_caption_data/local_run\\image_caption-10000\n",
      "INFO:tensorflow:[*] Success Read Checkpoint From image_caption-10000\n",
      "['2667078298.jpg']\n",
      "['A girl in a short v-neck blue dress and high heel sandals is carrying a '\n",
      " 'bouquet of calla lilies down and aisle with a man in a tuxedo .',\n",
      " 'A woman in a blue dress and a man in a suit walk down the aisle together at '\n",
      " 'a wedding ceremony .',\n",
      " 'A bridesmaid in a knee-length blue dress walks down the aisle with a '\n",
      " 'grooms-man .',\n",
      " 'A woman with flowers in her hand and a man walk on a walkway arm in arm .',\n",
      " 'A bridesmaid and groomsmen walk down the aisle .']\n",
      "[[3,\n",
      "  30,\n",
      "  4,\n",
      "  1,\n",
      "  464,\n",
      "  6627,\n",
      "  26,\n",
      "  117,\n",
      "  7,\n",
      "  292,\n",
      "  6683,\n",
      "  762,\n",
      "  8,\n",
      "  141,\n",
      "  1,\n",
      "  1556,\n",
      "  10,\n",
      "  0,\n",
      "  9989,\n",
      "  37,\n",
      "  7,\n",
      "  1874,\n",
      "  11,\n",
      "  1,\n",
      "  9,\n",
      "  4,\n",
      "  1,\n",
      "  2304,\n",
      "  2],\n",
      " [3,\n",
      "  13,\n",
      "  4,\n",
      "  1,\n",
      "  26,\n",
      "  117,\n",
      "  7,\n",
      "  1,\n",
      "  9,\n",
      "  4,\n",
      "  1,\n",
      "  191,\n",
      "  150,\n",
      "  37,\n",
      "  5,\n",
      "  1874,\n",
      "  140,\n",
      "  17,\n",
      "  1,\n",
      "  602,\n",
      "  1547,\n",
      "  2],\n",
      " [3, 5024, 4, 1, 0, 26, 117, 127, 37, 5, 1874, 11, 1, 0, 2],\n",
      " [3, 13, 11, 376, 4, 40, 131, 7, 1, 9, 150, 6, 1, 716, 349, 4, 349, 2],\n",
      " [3, 5024, 7, 7281, 150, 37, 5, 1874, 2]]\n",
      "'generated words: '\n",
      "[3, 39, 10, 19, 14, 32, 4, 1, 36, 12]\n",
      "'A group of people are standing in a street ,'\n"
     ]
    }
   ],
   "source": [
    "test_examples = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    logging.info(\"[*] Reading checkpoint ...\")\n",
    "    ckpt = tf.train.get_checkpoint_state(output_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(output_dir, ckpt_name))\n",
    "        logging.info(\"[*] Success Read Checkpoint From %s\" % (ckpt_name))\n",
    "    else:\n",
    "        raise Exception(\"[*] Failed load checkpoint\")\n",
    "    \n",
    "    for i in range(test_examples):\n",
    "        single_img_features, single_sentence_ids, single_weights, single_img_name = caption_data.next(hps.batch_size)\n",
    "        print(single_img_name)\n",
    "\n",
    "        pprint.pprint(img_name_to_tokens[single_img_name[0]])\n",
    "        pprint.pprint(img_name_to_token_ids[single_img_name[0]])\n",
    "\n",
    "        embed_img_val = sess.run(embed_img, feed_dict={img_feature: single_img_features})\n",
    "\n",
    "        state_val = np.zeros((1, num_hidden_states))\n",
    "        embed_input_val = embed_img_val\n",
    "        generated_sequence = []\n",
    "\n",
    "        for j in range(hps.num_timesteps):\n",
    "            logits_val, state_val = sess.run([logits, output_state],\n",
    "                                             feed_dict = {\n",
    "                                                 embed_input: embed_input_val,\n",
    "                                                 input_state: state_val\n",
    "                                             })\n",
    "            predicted_word_id = np.argmax(logits_val[0])\n",
    "            generated_sequence.append(predicted_word_id)\n",
    "            embed_input_val = sess.run(embed_word,\n",
    "                                       feed_dict={word: [[predicted_word_id]]})\n",
    "        pprint.pprint(\"generated words: \")\n",
    "        pprint.pprint(generated_sequence)\n",
    "        pprint.pprint(vocab.decode(generated_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他模型\n",
    "\n",
    "[show attention and tell](https://github.com/yunjey/show-attend-and-tell)\n",
    "\n",
    "[show attention and tell.tensorflow](https://github.com/jazzsaxmafia/show_attend_and_tell.tensorflow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
